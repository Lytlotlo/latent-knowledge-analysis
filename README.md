# latent-knowledge-analysis
This project investigates how knowledge is stored in language models (LLMs) by analyzing neural activations in response to factual vs. false information. It aims to explore whether models "know" the truth but fail to express it correctly. The work aligns with research in mechanistic interpretability and reverse engineering neural networks.
